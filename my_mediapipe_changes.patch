diff --git a/WORKSPACE b/WORKSPACE
index d4339488..f4d60c74 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -271,13 +271,13 @@ new_local_repository(
     # For local MacOS builds, the path should point to an opencv@3 installation.
     # If you edit the path here, you will also need to update the corresponding
     # prefix in "opencv_macos.BUILD".
-    path = "/usr/local",
+    path = "/opt/homebrew",
 )
 
 new_local_repository(
     name = "macos_ffmpeg",
     build_file = "@//third_party:ffmpeg_macos.BUILD",
-    path = "/usr/local/opt/ffmpeg",
+    path = "/opt/homebrew/opt/ffmpeg",
 )
 
 new_local_repository(
diff --git a/mediapipe/examples/desktop/gesture_tracking/BUILD b/mediapipe/examples/desktop/gesture_tracking/BUILD
new file mode 100644
index 00000000..a9722c16
--- /dev/null
+++ b/mediapipe/examples/desktop/gesture_tracking/BUILD
@@ -0,0 +1,71 @@
+licenses(["notice"])
+
+package(default_visibility = ["//mediapipe/examples:__subpackages__"])
+
+cc_binary(
+    name = "main",
+    srcs = ["main.cc"],
+    visibility = ["//visibility:public"],
+    linkshared = 0,
+    data = ["//mediapipe/modules/face_landmark:face_landmark_with_attention.tflite"],
+    deps = [
+        "//mediapipe/framework:calculator_framework",
+        "//mediapipe/framework/formats:image_frame",
+        "//mediapipe/framework/formats:image_frame_opencv",
+        "//mediapipe/framework/port:file_helpers",
+        "//mediapipe/framework/port:opencv_highgui",
+        "//mediapipe/framework/port:opencv_imgproc",
+        "//mediapipe/framework/port:opencv_video",
+        "//mediapipe/framework/port:parse_text_proto",
+        "//mediapipe/framework/port:status",
+        "@com_google_absl//absl/flags:flag",
+        "@com_google_absl//absl/flags:parse",
+        "//mediapipe/graphs/face_mesh:desktop_live_calculators",
+        # "@macos_opencv//:opencv",
+    ],
+)
+
+cc_library(
+    name = "facemesh_custom_lib",
+    srcs = ["facemesh_custom_lib.cc"],
+    hdrs = ["facemesh_custom_lib.h"],
+    visibility = ["//visibility:public"],
+    linkstatic = 1,
+    data = ["//mediapipe/modules/face_landmark:face_landmark_with_attention.tflite"],
+    deps = [
+        "//mediapipe/framework:calculator_framework",
+        "//mediapipe/framework/formats:image_frame",
+        "//mediapipe/framework/formats:image_frame_opencv",
+        "//mediapipe/framework/port:file_helpers",
+        "//mediapipe/framework/port:opencv_highgui",
+        "//mediapipe/framework/port:opencv_imgproc",
+        "//mediapipe/framework/port:opencv_video",
+        "//mediapipe/framework/port:parse_text_proto",
+        "//mediapipe/framework/port:status",
+        "@com_google_absl//absl/flags:flag",
+        "@com_google_absl//absl/flags:parse",
+        "//mediapipe/graphs/face_mesh:desktop_live_calculators",
+    ],
+)
+
+cc_binary(
+    name = "cleaned_up_demo",
+    srcs = ["cleaned_up_demo.cc"],
+    visibility = ["//visibility:public"],
+    data = ["//mediapipe/modules/face_landmark:face_landmark_with_attention.tflite"],
+    deps = [
+        "//mediapipe/framework:calculator_framework",
+        "//mediapipe/framework/formats:image_frame",
+        "//mediapipe/framework/formats:image_frame_opencv",
+        "//mediapipe/framework/port:file_helpers",
+        "//mediapipe/framework/port:opencv_highgui",
+        "//mediapipe/framework/port:opencv_imgproc",
+        "//mediapipe/framework/port:opencv_video",
+        "//mediapipe/framework/port:parse_text_proto",
+        "//mediapipe/framework/port:status",
+        "@com_google_absl//absl/flags:flag",
+        "@com_google_absl//absl/flags:parse",
+        "//mediapipe/graphs/face_mesh:desktop_live_calculators",
+        # "@macos_opencv//:opencv",
+    ],
+)
\ No newline at end of file
diff --git a/mediapipe/examples/desktop/gesture_tracking/cleaned_up_demo.cc b/mediapipe/examples/desktop/gesture_tracking/cleaned_up_demo.cc
new file mode 100644
index 00000000..2d495aa5
--- /dev/null
+++ b/mediapipe/examples/desktop/gesture_tracking/cleaned_up_demo.cc
@@ -0,0 +1,128 @@
+// Copyright 2019 The MediaPipe Authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//
+// An example of sending OpenCV webcam frames into a MediaPipe graph.
+#include <cstdlib>
+
+#include "absl/flags/flag.h"
+#include "absl/flags/parse.h"
+#include "mediapipe/framework/calculator_framework.h"
+#include "mediapipe/framework/formats/image_frame.h"
+#include "mediapipe/framework/formats/image_frame_opencv.h"
+#include "mediapipe/framework/port/file_helpers.h"
+#include "mediapipe/framework/port/opencv_highgui_inc.h"
+#include "mediapipe/framework/port/opencv_imgproc_inc.h"
+#include "mediapipe/framework/port/opencv_video_inc.h"
+#include "mediapipe/framework/port/parse_text_proto.h"
+#include "mediapipe/framework/port/status.h"
+
+constexpr char kInputStream[] = "input_video";
+constexpr char kOutputStream[] = "output_video";
+constexpr char kWindowName[] = "MediaPipe";
+
+ABSL_FLAG(std::string, calculator_graph_config_file, "",
+          "Name of file containing text format CalculatorGraphConfig proto.");
+
+absl::Status RunMPPGraph() {
+  std::string calculator_graph_config_contents;
+  MP_RETURN_IF_ERROR(mediapipe::file::GetContents(
+      absl::GetFlag(FLAGS_calculator_graph_config_file),
+      &calculator_graph_config_contents));
+  LOG(INFO) << "Get calculator graph config contents: "
+            << calculator_graph_config_contents;
+  mediapipe::CalculatorGraphConfig config =
+      mediapipe::ParseTextProtoOrDie<mediapipe::CalculatorGraphConfig>(
+          calculator_graph_config_contents);
+
+  LOG(INFO) << "Initialize the calculator graph.";
+  mediapipe::CalculatorGraph graph;
+  MP_RETURN_IF_ERROR(graph.Initialize(config));
+
+  LOG(INFO) << "Initialize the camera or load the video.";
+  cv::VideoCapture capture;
+  capture.open(0);
+  RET_CHECK(capture.isOpened());
+  
+  cv::namedWindow(kWindowName, /*flags=WINDOW_AUTOSIZE*/ 1);
+#if (CV_MAJOR_VERSION >= 3) && (CV_MINOR_VERSION >= 2)
+    capture.set(cv::CAP_PROP_FRAME_WIDTH, 640);
+    capture.set(cv::CAP_PROP_FRAME_HEIGHT, 480);
+    capture.set(cv::CAP_PROP_FPS, 30);
+#endif
+
+  LOG(INFO) << "Start running the calculator graph.";
+  ASSIGN_OR_RETURN(mediapipe::OutputStreamPoller poller,
+                   graph.AddOutputStreamPoller(kOutputStream));
+  MP_RETURN_IF_ERROR(graph.StartRun({}));
+
+  LOG(INFO) << "Start grabbing and processing frames.";
+  bool grab_frames = true;
+  while (grab_frames) {
+    // Capture opencv camera or video frame.
+    cv::Mat camera_frame_raw;
+    capture >> camera_frame_raw;
+    if (camera_frame_raw.empty()) {
+      LOG(INFO) << "Ignore empty frames from camera.";
+      continue;
+    }
+    cv::Mat camera_frame;
+    cv::cvtColor(camera_frame_raw, camera_frame, cv::COLOR_BGR2RGB);
+    cv::flip(camera_frame, camera_frame, /*flipcode=HORIZONTAL*/ 1);
+
+    // Wrap Mat into an ImageFrame.
+    auto input_frame = absl::make_unique<mediapipe::ImageFrame>(
+        mediapipe::ImageFormat::SRGB, camera_frame.cols, camera_frame.rows,
+        mediapipe::ImageFrame::kDefaultAlignmentBoundary);
+    cv::Mat input_frame_mat = mediapipe::formats::MatView(input_frame.get());
+    camera_frame.copyTo(input_frame_mat);
+
+    // Send image packet into the graph.
+    size_t frame_timestamp_us =
+        (double)cv::getTickCount() / (double)cv::getTickFrequency() * 1e6;
+    MP_RETURN_IF_ERROR(graph.AddPacketToInputStream(
+        kInputStream, mediapipe::Adopt(input_frame.release())
+                          .At(mediapipe::Timestamp(frame_timestamp_us))));
+
+    // Get the graph result packet, or stop if that fails.
+    mediapipe::Packet packet;
+    if (!poller.Next(&packet)) break;
+    auto& output_frame = packet.Get<mediapipe::ImageFrame>();
+
+    // Convert back to opencv for display or saving.
+    cv::Mat output_frame_mat = mediapipe::formats::MatView(&output_frame);
+    cv::cvtColor(output_frame_mat, output_frame_mat, cv::COLOR_RGB2BGR);
+
+    cv::imshow(kWindowName, output_frame_mat);
+    // Press any key to exit.
+    const int pressed_key = cv::waitKey(5);
+    if (pressed_key >= 0 && pressed_key != 255) grab_frames = false;
+  }
+
+  LOG(INFO) << "Shutting down.";
+  MP_RETURN_IF_ERROR(graph.CloseInputStream(kInputStream));
+  return graph.WaitUntilDone();
+}
+
+int main(int argc, char** argv) {
+  google::InitGoogleLogging(argv[0]);
+  absl::ParseCommandLine(argc, argv);
+  absl::Status run_status = RunMPPGraph();
+  if (!run_status.ok()) {
+    LOG(ERROR) << "Failed to run the graph: " << run_status.message();
+    return EXIT_FAILURE;
+  } else {
+    LOG(INFO) << "Success!";
+  }
+  return EXIT_SUCCESS;
+}
diff --git a/mediapipe/examples/desktop/gesture_tracking/face_mesh_desktop_live_custom.pbtxt b/mediapipe/examples/desktop/gesture_tracking/face_mesh_desktop_live_custom.pbtxt
new file mode 100644
index 00000000..2cc56342
--- /dev/null
+++ b/mediapipe/examples/desktop/gesture_tracking/face_mesh_desktop_live_custom.pbtxt
@@ -0,0 +1,66 @@
+# MediaPipe graph that performs face mesh with TensorFlow Lite on CPU.
+
+# Input image. (ImageFrame)
+input_stream: "input_video"
+
+# Output image with rendered results. (ImageFrame)
+output_stream: "output_video"
+# Collection of detected/processed faces, each represented as a list of
+# landmarks. (std::vector<NormalizedLandmarkList>)
+output_stream: "multi_face_landmarks"
+
+# Throttles the images flowing downstream for flow control. It passes through
+# the very first incoming image unaltered, and waits for downstream nodes
+# (calculators and subgraphs) in the graph to finish their tasks before it
+# passes through another image. All images that come in while waiting are
+# dropped, limiting the number of in-flight images in most part of the graph to
+# 1. This prevents the downstream nodes from queuing up incoming images and data
+# excessively, which leads to increased latency and memory usage, unwanted in
+# real-time mobile applications. It also eliminates unnecessarily computation,
+# e.g., the output produced by a node may get dropped downstream if the
+# subsequent nodes are still busy processing previous inputs.
+node {
+  calculator: "FlowLimiterCalculator"
+  input_stream: "input_video"
+  input_stream: "FINISHED:output_video"
+  input_stream_info: {
+    tag_index: "FINISHED"
+    back_edge: true
+  }
+  output_stream: "throttled_input_video"
+}
+
+# Defines side packets for further use in the graph.
+node {
+  calculator: "ConstantSidePacketCalculator"
+  output_side_packet: "PACKET:0:num_faces"
+  output_side_packet: "PACKET:1:with_attention"
+  node_options: {
+    [type.googleapis.com/mediapipe.ConstantSidePacketCalculatorOptions]: {
+      packet { int_value: 1 }
+      packet { bool_value: true }
+    }
+  }
+}
+
+# Subgraph that detects faces and corresponding landmarks.
+node {
+  calculator: "FaceLandmarkFrontCpu"
+  input_stream: "IMAGE:throttled_input_video"
+  input_side_packet: "NUM_FACES:num_faces"
+  input_side_packet: "WITH_ATTENTION:with_attention"
+  output_stream: "LANDMARKS:multi_face_landmarks"
+  output_stream: "ROIS_FROM_LANDMARKS:face_rects_from_landmarks"
+  output_stream: "DETECTIONS:face_detections"
+  output_stream: "ROIS_FROM_DETECTIONS:face_rects_from_detections"
+}
+
+# Subgraph that renders face-landmark annotation onto the input image.
+node {
+  calculator: "FaceRendererCpu"
+  input_stream: "IMAGE:throttled_input_video"
+  input_stream: "LANDMARKS:multi_face_landmarks"
+  input_stream: "NORM_RECTS:face_rects_from_landmarks"
+  input_stream: "DETECTIONS:face_detections"
+  output_stream: "IMAGE:output_video"
+}
diff --git a/mediapipe/examples/desktop/gesture_tracking/facemesh_custom_lib.cc b/mediapipe/examples/desktop/gesture_tracking/facemesh_custom_lib.cc
new file mode 100644
index 00000000..1b739479
--- /dev/null
+++ b/mediapipe/examples/desktop/gesture_tracking/facemesh_custom_lib.cc
@@ -0,0 +1,145 @@
+#include "facemesh_custom_lib.h"
+#include <cstdlib>
+#include "absl/flags/flag.h"
+#include "absl/flags/parse.h"
+#include "mediapipe/framework/calculator_framework.h"
+#include "mediapipe/framework/formats/image_frame.h"
+#include "mediapipe/framework/formats/image_frame_opencv.h"
+#include "mediapipe/framework/formats/landmark.pb.h"
+#include "mediapipe/framework/port/file_helpers.h"
+#include "mediapipe/framework/port/opencv_highgui_inc.h"
+#include "mediapipe/framework/port/opencv_imgproc_inc.h"
+#include "mediapipe/framework/port/opencv_video_inc.h"
+#include "mediapipe/framework/port/parse_text_proto.h"
+#include "mediapipe/framework/port/status.h"
+
+// https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png
+enum FaceMeshPoint {
+  right_eye_corner = 33,
+  left_eye_corner = 263,
+  nose_tip = 1,
+  between_eyes = 6,
+  right_mouth_corner = 61,
+  left_mouth_corner = 291,
+  chin_middle = 199,
+};
+
+constexpr char kInputStream[] = "input_video";
+constexpr char kOutputStreamImage[] = "output_video";
+constexpr char kOutputStreamLandmark[] = "multi_face_landmarks";
+constexpr char kWindowName[] = "MediaPipe";
+
+ABSL_FLAG(std::string, calculator_graph_config_file, "",
+          "Name of file containing text format CalculatorGraphConfig proto.");
+
+absl::Status RunMPPGraph() {
+  std::string calculator_graph_config_contents;
+  MP_RETURN_IF_ERROR(mediapipe::file::GetContents(
+      absl::GetFlag(FLAGS_calculator_graph_config_file),
+      &calculator_graph_config_contents));
+  LOG(INFO) << "Get calculator graph config contents: "
+            << calculator_graph_config_contents;
+  mediapipe::CalculatorGraphConfig config =
+      mediapipe::ParseTextProtoOrDie<mediapipe::CalculatorGraphConfig>(
+          calculator_graph_config_contents);
+
+  LOG(INFO) << "Initialize the calculator graph.";
+  mediapipe::CalculatorGraph graph;
+  MP_RETURN_IF_ERROR(graph.Initialize(config));
+
+  LOG(INFO) << "Initialize the camera or load the video.";
+  cv::VideoCapture capture;
+  capture.open(0);
+  RET_CHECK(capture.isOpened());
+  
+  cv::namedWindow(kWindowName, /*flags=WINDOW_AUTOSIZE*/ 1);
+#if (CV_MAJOR_VERSION >= 3) && (CV_MINOR_VERSION >= 2)
+    capture.set(cv::CAP_PROP_FRAME_WIDTH, 640);
+    capture.set(cv::CAP_PROP_FRAME_HEIGHT, 480);
+    capture.set(cv::CAP_PROP_FPS, 30);
+#endif
+
+  LOG(INFO) << "Start running the calculator graph.";
+  ASSIGN_OR_RETURN(mediapipe::OutputStreamPoller image_poller,
+                   graph.AddOutputStreamPoller(kOutputStreamImage));
+  ASSIGN_OR_RETURN(mediapipe::OutputStreamPoller landmark_poller,
+                   graph.AddOutputStreamPoller(kOutputStreamLandmark));
+  MP_RETURN_IF_ERROR(graph.StartRun({}));
+
+  LOG(INFO) << "Start grabbing and processing frames.";
+  bool grab_frames = true;
+  while (grab_frames) {
+    // Capture opencv camera or video frame.
+    cv::Mat camera_frame_raw;
+    capture >> camera_frame_raw;
+    if (camera_frame_raw.empty()) {
+      LOG(INFO) << "Ignore empty frames from camera.";
+      continue;
+    }
+    cv::Mat camera_frame;
+    cv::cvtColor(camera_frame_raw, camera_frame, cv::COLOR_BGR2RGB);
+    cv::flip(camera_frame, camera_frame, /*flipcode=HORIZONTAL*/ 1);
+
+    // Wrap Mat into an ImageFrame.
+    auto input_frame = absl::make_unique<mediapipe::ImageFrame>(
+        mediapipe::ImageFormat::SRGB, camera_frame.cols, camera_frame.rows,
+        mediapipe::ImageFrame::kDefaultAlignmentBoundary);
+    cv::Mat input_frame_mat = mediapipe::formats::MatView(input_frame.get());
+    camera_frame.copyTo(input_frame_mat);
+
+    // Send image packet into the graph.
+    size_t frame_timestamp_us =
+        (double)cv::getTickCount() / (double)cv::getTickFrequency() * 1e6;
+    MP_RETURN_IF_ERROR(graph.AddPacketToInputStream(
+        kInputStream, mediapipe::Adopt(input_frame.release())
+                          .At(mediapipe::Timestamp(frame_timestamp_us))));
+
+    // Get the graph result packet, or stop if that fails.
+    mediapipe::Packet image_packet;
+    if (!image_poller.Next(&image_packet)) break;
+    auto& output_frame = image_packet.Get<mediapipe::ImageFrame>();
+
+    // When we grab the image data before the landmarks data, if there are any faces detected
+    // the queue size of the landmark data will be there already, so we don't get stuck in 
+    // synchronous .Next() call in case there is no data to be taken.
+    std::vector<mediapipe::NormalizedLandmarkList> output_landmarks;
+    int landmarks_present = landmark_poller.QueueSize();
+
+    if (landmarks_present) {
+      mediapipe::Packet landmark_packet;
+      if (!landmark_poller.Next(&landmark_packet)) break;
+      output_landmarks = landmark_packet.Get<std::vector<mediapipe::NormalizedLandmarkList>>();
+    }
+
+    if (output_landmarks.size() > 0) {
+      mediapipe::NormalizedLandmark lm = output_landmarks.front().landmark(FaceMeshPoint::between_eyes);
+      LOG(INFO) << "between eyes (x, y, z): (" << lm.x() << ", " << lm.y() << ", " << lm.z() << ")";
+    }
+
+    // Convert back to opencv for display or saving.
+    cv::Mat output_frame_mat = mediapipe::formats::MatView(&output_frame);
+    cv::cvtColor(output_frame_mat, output_frame_mat, cv::COLOR_RGB2BGR);
+
+    cv::imshow(kWindowName, output_frame_mat);
+    // Press any key to exit.
+    const int pressed_key = cv::waitKey(5);
+    if (pressed_key >= 0 && pressed_key != 255) grab_frames = false;
+  }
+
+  LOG(INFO) << "Shutting down.";
+  MP_RETURN_IF_ERROR(graph.CloseInputStream(kInputStream));
+  return graph.WaitUntilDone();
+}
+
+int RunDemo(int argc, char** argv) {
+  google::InitGoogleLogging(argv[0]);
+  absl::ParseCommandLine(argc, argv);
+  absl::Status run_status = RunMPPGraph();
+  if (!run_status.ok()) {
+    LOG(ERROR) << "Failed to run the graph: " << run_status.message();
+    return EXIT_FAILURE;
+  } else {
+    LOG(INFO) << "Success!";
+  }
+  return EXIT_SUCCESS;
+}
\ No newline at end of file
diff --git a/mediapipe/examples/desktop/gesture_tracking/facemesh_custom_lib.h b/mediapipe/examples/desktop/gesture_tracking/facemesh_custom_lib.h
new file mode 100644
index 00000000..d3e6f3e8
--- /dev/null
+++ b/mediapipe/examples/desktop/gesture_tracking/facemesh_custom_lib.h
@@ -0,0 +1,6 @@
+#ifndef FACEMESH_CUSTOM_LIB_H
+#define FACEMESH_CUSTOM_LIB_H
+
+int RunDemo(int argc, char** argv);
+
+#endif // FACEMESH_CUSTOM_LIB_H
\ No newline at end of file
diff --git a/mediapipe/examples/desktop/gesture_tracking/lib_usage.cc b/mediapipe/examples/desktop/gesture_tracking/lib_usage.cc
new file mode 100644
index 00000000..19f3b8dc
--- /dev/null
+++ b/mediapipe/examples/desktop/gesture_tracking/lib_usage.cc
@@ -0,0 +1,7 @@
+#include <iostream>
+#include "facemesh_custom_lib.h"
+
+int main(int argc, char** argv) {
+    std::cout << "Invoking custom lib including mediapipe!" << std::endl;
+    return RunDemo(argc, argv);
+}
\ No newline at end of file
diff --git a/mediapipe/examples/desktop/gesture_tracking/main.cc b/mediapipe/examples/desktop/gesture_tracking/main.cc
new file mode 100644
index 00000000..61f8c20a
--- /dev/null
+++ b/mediapipe/examples/desktop/gesture_tracking/main.cc
@@ -0,0 +1,160 @@
+// Copyright 2019 The MediaPipe Authors.
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//
+// An example of sending OpenCV webcam frames into a MediaPipe graph.
+#include <cstdlib>
+
+#include "absl/flags/flag.h"
+#include "absl/flags/parse.h"
+#include "mediapipe/framework/calculator_framework.h"
+#include "mediapipe/framework/formats/image_frame.h"
+#include "mediapipe/framework/formats/image_frame_opencv.h"
+#include "mediapipe/framework/formats/landmark.pb.h"
+#include "mediapipe/framework/port/file_helpers.h"
+#include "mediapipe/framework/port/opencv_highgui_inc.h"
+#include "mediapipe/framework/port/opencv_imgproc_inc.h"
+#include "mediapipe/framework/port/opencv_video_inc.h"
+#include "mediapipe/framework/port/parse_text_proto.h"
+#include "mediapipe/framework/port/status.h"
+
+// https://raw.githubusercontent.com/google/mediapipe/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png
+enum FaceMeshPoint {
+  right_eye_corner = 33,
+  left_eye_corner = 263,
+  nose_tip = 1,
+  between_eyes = 6,
+  right_mouth_corner = 61,
+  left_mouth_corner = 291,
+  chin_middle = 199,
+};
+
+constexpr char kInputStream[] = "input_video";
+constexpr char kOutputStreamImage[] = "output_video";
+constexpr char kOutputStreamLandmark[] = "multi_face_landmarks";
+constexpr char kWindowName[] = "MediaPipe";
+
+ABSL_FLAG(std::string, calculator_graph_config_file, "",
+          "Name of file containing text format CalculatorGraphConfig proto.");
+
+absl::Status RunMPPGraph() {
+  std::string calculator_graph_config_contents;
+  MP_RETURN_IF_ERROR(mediapipe::file::GetContents(
+      absl::GetFlag(FLAGS_calculator_graph_config_file),
+      &calculator_graph_config_contents));
+  LOG(INFO) << "Get calculator graph config contents: "
+            << calculator_graph_config_contents;
+  mediapipe::CalculatorGraphConfig config =
+      mediapipe::ParseTextProtoOrDie<mediapipe::CalculatorGraphConfig>(
+          calculator_graph_config_contents);
+
+  LOG(INFO) << "Initialize the calculator graph.";
+  mediapipe::CalculatorGraph graph;
+  MP_RETURN_IF_ERROR(graph.Initialize(config));
+
+  LOG(INFO) << "Initialize the camera or load the video.";
+  cv::VideoCapture capture;
+  capture.open(0);
+  RET_CHECK(capture.isOpened());
+  
+  cv::namedWindow(kWindowName, /*flags=WINDOW_AUTOSIZE*/ 1);
+#if (CV_MAJOR_VERSION >= 3) && (CV_MINOR_VERSION >= 2)
+    capture.set(cv::CAP_PROP_FRAME_WIDTH, 640);
+    capture.set(cv::CAP_PROP_FRAME_HEIGHT, 480);
+    capture.set(cv::CAP_PROP_FPS, 30);
+#endif
+
+  LOG(INFO) << "Start running the calculator graph.";
+  ASSIGN_OR_RETURN(mediapipe::OutputStreamPoller image_poller,
+                   graph.AddOutputStreamPoller(kOutputStreamImage));
+  ASSIGN_OR_RETURN(mediapipe::OutputStreamPoller landmark_poller,
+                   graph.AddOutputStreamPoller(kOutputStreamLandmark));
+  MP_RETURN_IF_ERROR(graph.StartRun({}));
+
+  LOG(INFO) << "Start grabbing and processing frames.";
+  bool grab_frames = true;
+  while (grab_frames) {
+    // Capture opencv camera or video frame.
+    cv::Mat camera_frame_raw;
+    capture >> camera_frame_raw;
+    if (camera_frame_raw.empty()) {
+      LOG(INFO) << "Ignore empty frames from camera.";
+      continue;
+    }
+    cv::Mat camera_frame;
+    cv::cvtColor(camera_frame_raw, camera_frame, cv::COLOR_BGR2RGB);
+    cv::flip(camera_frame, camera_frame, /*flipcode=HORIZONTAL*/ 1);
+
+    // Wrap Mat into an ImageFrame.
+    auto input_frame = absl::make_unique<mediapipe::ImageFrame>(
+        mediapipe::ImageFormat::SRGB, camera_frame.cols, camera_frame.rows,
+        mediapipe::ImageFrame::kDefaultAlignmentBoundary);
+    cv::Mat input_frame_mat = mediapipe::formats::MatView(input_frame.get());
+    camera_frame.copyTo(input_frame_mat);
+
+    // Send image packet into the graph.
+    size_t frame_timestamp_us =
+        (double)cv::getTickCount() / (double)cv::getTickFrequency() * 1e6;
+    MP_RETURN_IF_ERROR(graph.AddPacketToInputStream(
+        kInputStream, mediapipe::Adopt(input_frame.release())
+                          .At(mediapipe::Timestamp(frame_timestamp_us))));
+
+    // Get the graph result packet, or stop if that fails.
+    mediapipe::Packet image_packet;
+    if (!image_poller.Next(&image_packet)) break;
+    auto& output_frame = image_packet.Get<mediapipe::ImageFrame>();
+
+    // When we grab the image data before the landmarks data, if there are any faces detected
+    // the queue size of the landmark data will be there already, so we don't get stuck in 
+    // synchronous .Next() call in case there is no data to be taken.
+    std::vector<mediapipe::NormalizedLandmarkList> output_landmarks;
+    int landmarks_present = landmark_poller.QueueSize();
+
+    if (landmarks_present) {
+      mediapipe::Packet landmark_packet;
+      if (!landmark_poller.Next(&landmark_packet)) break;
+      output_landmarks = landmark_packet.Get<std::vector<mediapipe::NormalizedLandmarkList>>();
+    }
+
+    if (output_landmarks.size() > 0) {
+      mediapipe::NormalizedLandmark lm = output_landmarks.front().landmark(FaceMeshPoint::between_eyes);
+      LOG(INFO) << "between eyes (x, y, z): (" << lm.x() << ", " << lm.y() << ", " << lm.z() << ")";
+    }
+
+    // Convert back to opencv for display or saving.
+    cv::Mat output_frame_mat = mediapipe::formats::MatView(&output_frame);
+    cv::cvtColor(output_frame_mat, output_frame_mat, cv::COLOR_RGB2BGR);
+
+    cv::imshow(kWindowName, output_frame_mat);
+    // Press any key to exit.
+    const int pressed_key = cv::waitKey(5);
+    if (pressed_key >= 0 && pressed_key != 255) grab_frames = false;
+  }
+
+  LOG(INFO) << "Shutting down.";
+  MP_RETURN_IF_ERROR(graph.CloseInputStream(kInputStream));
+  return graph.WaitUntilDone();
+}
+
+int main(int argc, char** argv) {
+  google::InitGoogleLogging(argv[0]);
+  absl::ParseCommandLine(argc, argv);
+  absl::Status run_status = RunMPPGraph();
+  if (!run_status.ok()) {
+    LOG(ERROR) << "Failed to run the graph: " << run_status.message();
+    return EXIT_FAILURE;
+  } else {
+    LOG(INFO) << "Success!";
+  }
+  return EXIT_SUCCESS;
+}
diff --git a/mediapipe/examples/desktop/gesture_tracking/scripts.txt b/mediapipe/examples/desktop/gesture_tracking/scripts.txt
new file mode 100644
index 00000000..639c14be
--- /dev/null
+++ b/mediapipe/examples/desktop/gesture_tracking/scripts.txt
@@ -0,0 +1,23 @@
+Main:
+
+bazel build --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/gesture_tracking:main
+
+GLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/gesture_tracking/main --calculator_graph_config_file=mediapipe/examples/desktop/gesture_tracking/face_mesh_desktop_live_custom.pbtxt
+
+
+
+Lib usage:
+
+bazel build --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/gesture_tracking:facemesh_custom_lib
+
+clang++ mediapipe/examples/desktop/gesture_tracking/lib_usage.cc -arch arm64 -L "bazel-bin/mediapipe/examples/desktop/gesture_tracking" -lfacemesh_custom_lib -o mediapipe/examples/desktop/gesture_tracking/lib_usage
+
+
+
+Cleaned up demo:
+
+bazel run --define MEDIAPIPE_DISABLE_GPU=1 mediapipe/examples/desktop/gesture_tracking:cleaned_up_demo
+
+GLOG_logtostderr=1 bazel-bin/mediapipe/examples/desktop/gesture_tracking/cleaned_up_demo --calculator_graph_config_file=mediapipe/graphs/face_mesh/face_mesh_desktop_live.pbtxt
+
+GLOG_logtostderr=1 ./main --calculator_graph_config_file=./face_mesh_desktop_live_custom.pbtxt
diff --git a/mediapipe/modules/face_detection/face_detection_short_range.tflite b/mediapipe/modules/face_detection/face_detection_short_range.tflite
new file mode 100644
index 00000000..659bce89
Binary files /dev/null and b/mediapipe/modules/face_detection/face_detection_short_range.tflite differ
